
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>1. Introduction to Kubernetes with Namespaces</title>
  <script src="../bower_components/webcomponentsjs/webcomponents-lite.js"></script>
  <link rel="import" href="../elements/codelab.html">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <style is="custom-style">
    body {
      font-family: "Roboto",sans-serif;
      background: var(--google-codelab-background, #F8F9FA);
    }
  </style>
  
</head>
<body unresolved class="fullbleed">

  <google-codelab title="1. Introduction to Kubernetes with Namespaces"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="1. Install Google Cloud SDK tool" duration="0">
        <p>In order to explore the Kubernetes cluster on Google Kubernetes Engine you need to install the Google Cloud SDK command line tool.</p>
<ol type="1">
<li>Follow the guide to setup the <code>gcloud</code> tool, but stop before the step <code>gcloud init</code>. You can find the guide <a href="https://cloud.google.com/sdk/docs/downloads-interactive" target="_blank">here</a></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="2. Activate service account" duration="0">
        <p>Email <code>linemos@gmail.com</code> with the topic <code>Kubernetes intro SA</code> to create a service account.</p>
<p>When you have received an service account, download the file. We will use it to authenticate with Google Cloud.</p>
<ol type="1">
<li>Open the file and copy the email adress in the field <code>client_email</code></li>
<li>Use this command to authenticate your computer with the cluster:<pre><code>gcloud auth activate-service-account INSERT_CLIENT_EMAIL_HERE --key-file=PATH_TO_JSON_FILE --project INSERT_PROJECT_NAME    
</code></pre>
</li>
<li>Verify that you have successfully authenticated by this command:<pre><code>gcloud container clusters list
</code></pre>
The result should be similar to this:<pre><code>NAME        LOCATION         MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS 
cv-cluster  europe-north1-a  1.10.2-gke.3    35.197.214.235  n1-standard-2  1.10.2-gke.3  6          RUNNING
</code></pre>
</li>
</ol>
<h2>1. Describe components of the cluster</h2>
<p>Now that we are authenticated, we can look at the components in our cluster by using the kubectl command.</p>
<ol type="1">
<li>Remember how Kubernetes consists of nodes? List them by this command:<pre><code>kubectl get nodes
</code></pre>
</li>
<li>If you want you can get more details about them by describing one of them:<pre><code>kubectl describe nodes &lt;INSERT_NODE_NAME&gt;
</code></pre>
</li>
</ol>
<p>A node is a worker machine in Kubernetes. A node may be a VM or physical machine, depending on the cluster.</p>


      </google-codelab-step>
    
      <google-codelab-step label="3. Clone this repository" duration="0">
        <ol type="1">
<li>Clone <a href="https://github.com/linemos/kubernetes-intro" target="_blank">this</a> repository to your laptop.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="4. Deploy to your Kubernetes Cluster" duration="0">
        <p>It&#39;s time to deploy the frontend and backend to your cluster!<br>The preferred way to configure Kubernetes resources is to specify them in YAML files.</p>
<p>In the folder <a href="../../yaml" target="_blank">yaml/</a> you find the YAML files specifying what resources Kubernetes should create.<br>There are two services, one for the backend application and one for the frontend application.<br>Same for the deployments.</p>
<ol type="1">
<li>Open the file <a href="../../yaml/backend-deployment.yaml" target="_blank">yaml/backend-deployment.yaml</a> and<br>in the field <code>spec.template.spec.containers.image</code> insert the path to the Docker image we have created for the backend: <code>gcr.io/ndc-london-kubernetes/backend:1</code>.</li>
</ol>
<p>There are a few things to notice in the deployment file:</p>
<ul>
<li>The number of replicas is set to 3. This is the number of pods we want running at all times</li>
<li>The container spec has defined port 5000, so the Deployment will open this port on the containers</li>
<li>The label <code>app: backend</code> is defined three places:<br><br><ul>
<li><code>metadata</code> is used by the service, which we will look at later</li>
<li><code>spec.selector.matchLabels</code> is how the Deployment knows which Pods to manage</li>
<li><code>spec.template.metadata</code> is the label added to the Pods<br><br></li>
</ul>
</li>
</ul>
<ol type="1">
<li>Open the file <a href="../../yaml/frontend-deployment.yaml" target="_blank">yaml/frontend-deployment.yaml</a> and<br>in the field <code>spec.template.spec.containers.image</code> insert <code>gcr.io/ndc-london-kubernetes/frontend:1</code>, which is a Docker image we have created for the frontend application.</li>
<li>Create the resources for the backend and frontend (from root folder in the project):<pre><code>kubectl apply -f ./yaml/backend-deployment.yaml
kubectl apply -f ./yaml/frontend-deployment.yaml
</code></pre>
</li>
<li>Watch the creation of pods:<pre><code>watch kubectl get pods
</code></pre>
</li>
</ol>
<p>If you don&#39;t have <code>watch</code> installed, you can use this command instead:</p>
<pre><code>  kubectl get pods -w
</code></pre>
<p>When all pods are running, quit by <code>ctrl + q</code> (or <code>ctrl + c</code> when on Windows).</p>
<p>Pods are Kubernetes resources that mostly just contains one or more containers,<br>along with some Kubernetes network stuff and specifications on how to run the container(s).<br>Our pods all just contains one container. There are several use cases where you might want to specify several<br>containers in one pod, for example if your application is using a proxy.</p>
<p>The Pods were created when you applied the specification of the type <code>Deployment</code>,<br>which is a controller resource.<br>The Deployment specification contains a desired state and the Deployment controller changes the state to achieve this.<br>When creating the Deployment, it will create ReplicaSet, which it owns.<br>The ReplicaSet will then create the desired number of pods, and recreate them if the Deployment specification changes,<br>e.g. if you want another number of pods running or if you update the Docker image to use.<br>It will do so in a rolling-update manner, which we will explore soon.</p>
<p><em>Did you noticed that the pod names were prefixed with the deployment names and two hashes?</em> - The first hash is the hash of the ReplicaSet, the second is unique for the Pod.</p>
<ol type="1">
<li>Explore the Deployments:<pre><code>kubectl get deployments
</code></pre>
</li>
</ol>
<p>Here you can see the age of the Deployment and how many Pods that are desired in the configuration specification,<br>the number of running pods, the number of pods that are updated and how many that are available.</p>
<ol type="1">
<li>Explore the ReplicaSets:<pre><code>kubectl get replicaset
</code></pre>
</li>
</ol>
<p>The statuses are similar to those of the Deployments, except that the ReplicaSet have no concept of updates.<br>If you run an update to a Deployment, it will create a new ReplicaSet with the updated specification and<br>tell the old ReplicaSet to scale number of pods down to zero.</p>
<h2>1. Create services</h2>
<p>Now that our applications are running, we would like to route traffic to them.</p>
<ol type="1">
<li>Open <a href="../../yaml/backend-service.yaml" target="_blank">yaml/backend-service.yaml</a><br>There are a few things to notice:<ul>
<li>The protocol is set to TCP, which means that the Service sends requests to Pods on this protocol. UDP is also supported</li>
<li>The spec has defined port 80, so it will listen to traffic on port 80 and sends traffic to the Pods on the same port. We could also define <code>targetPort</code> if the port on the Pods are different from the incoming traffic port</li>
<li>The label <code>app: backend</code> defines that it should route requests to our Deployment with the same label</li>
</ul>
</li>
<li>Create the Services:<pre><code>kubectl apply -f ./yaml/backend-service.yaml
kubectl apply -f ./yaml/frontend-service.yaml
</code></pre>
</li>
<li>List the created services:<pre><code>kubectl get service
</code></pre>
</li>
</ol>
<p>As you can see, both services have defined internal IPs, <code>CLUSTER-IP</code>. These internal IPs are only available inside the cluster. But we want our frontend application to be available from the internet. In order to do so, we must expose an external IP.</p>
<h2>2. Exposing your app</h2>
<p>Ok, so now what? With the previous command, we saw that we had two services, one for our frontend and one for our backend. But they both had internal IPs, no external. We want to be able to browse our application from our browser.<br>Lets look at another way. The Service resource can have a different type, it can be set as a LoadBalancer.</p>
<ol type="1">
<li>Open the frontend service file again</li>
<li>Set <code>type</code> to be <code>LoadBalancer</code></li>
<li>Save and run<pre><code>kubectl apply -f ./yaml/frontend-service.yaml
</code></pre>
</li>
<li>Wait for an external IP:<pre><code>watch kubectl get service frontend
</code></pre>
</li>
</ol>
<p>or</p>
<pre><code>  kubectl get service frontend -w
</code></pre>
<ol type="1">
<li>Visit the IP in your browser to see your amazing CV online. But something is off!<br>There is no data, and if you inspect the network traffic in the browser console log, you can see that the requests to the api is responding with an error code.This is because the frontend application is expecting the IP of the backend Service to be set at the point of deployment.<br>But since we deployed the frontend application before creating the Service objects,<br>meaning there was not any IP to give the frontend container on creation time.</li>
<li>To fix this, we can delete the ReplicaSet for the frontend application:<pre><code>kubectl delete replicaset -l app=frontend
</code></pre>
By doing this, the Deployment will create a new ReplicaSet which will again create new Pods.<br>At this time the backend Service exists and is given to the frontend application.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="5. Rolling updates" duration="0">
        <p>As you read earlier, Kubernetes can update your application without down time with a rolling-update strategy.<br>You will now update the background color of the frontend application, see that the build trigger creates a new image and<br>update the deployment to use this in your web application.</p>
<ol type="1">
<li>Update the image specification on the file <a href="../../yaml/frontend-deployment.yaml" target="_blank">yaml/frontend-deployment.yaml</a> by adding the tag <code>:2</code></li>
<li>Open a new terminal window to watch the deletion and creation of Pods:<pre><code>watch kubectl get pods
</code></pre>
</li>
</ol>
<p>If you don&#39;t have <code>watch</code> installed, you can use this command instead:</p>
<pre><code>  kubectl get pods -w
</code></pre>
<p>Don&#39;t close this window.</p>
<ol type="1">
<li>In the other terminal window, apply the updated Deployment specification<pre><code>kubectl apply -f ./yaml/frontend-deployment.yaml
</code></pre>
</li>
</ol>
<p>and watch how the Pods are terminated and created in the other terminal window.<br>Notice that there are always at least one Pod running and that the last of the old Pods are first terminated when on of the new ones has the status running.</p>


      </google-codelab-step>
    
      <google-codelab-step label="6. Extra tasks" duration="0">
        <h2>1. Different methods to expose a service</h2>
<p>Right now we have exposed our frontend service through an ingress. We will now look into two other ways.</p>
<h3>1. Service type NodePort</h3>
<p>The first way is with the service type NodePort. If we look at our frontend service, we can see that it already is defined as this type. So we are good to go then? No, not yet.</p>
<ol type="1">
<li>We will change our frontend service to be a type NodePort instead. Open the file <a href="../../yaml/frontend-service.yaml" target="_blank">yaml/frontend-service.yaml</a></li>
<li>Set the <code>type</code> to be <code>NodePort</code> and save</li>
<li>Apply the changes<pre><code>kubectl apply -f ./yaml/frontend-service.yaml
</code></pre>
</li>
<li>Run<pre><code>kubectl get service frontend
</code></pre>
</li>
</ol>
<p>We see that our service doesn&#39;t have an external IP. But what it do have is two ports, port 80 and a port in the range 30000-32767. The last port was set by the Kubernetes master when we created our service. This port we will use togheter with an external IP.</p>
<ol type="1">
<li>The nodes in our cluster all have external IPs per default. Lets use one of those.<pre><code>kubectl get nodes -o wide
</code></pre>
</li>
<li>Copy one of the external IPs from the output above along with the node port from our service:<pre><code>curl -v &lt;EXTERNAL_IP&gt;:&lt;NODE_PORT&gt;
</code></pre>
</li>
</ol>
<p>This will output <code>Connection failed</code>. This is because we haven&#39;t opened up requests on this port. Lets create a firewall rule that allows traffic on this port:</p>
<ol type="1">
<li>Create a firewall rule. Switch <code>NODE_PORT</code> with the node port of your service:<pre><code>gcloud compute firewall-rules create cv-frontend --allow tcp:NODE_PORT
</code></pre>
</li>
<li>Try the curl command from <code>6</code> again.<br><br>The output should also here be &#34;Hello, I&#39;m alive&#34;</li>
<li>Do the same, but replace the IP with the external IP from one of the other nodes. It should have the same result</li>
</ol>
<p>How does this work? The nodes all have external IPs, so we can curl them. By default, neither services or pods in the cluster are exposed to the internet, but Kubernetes will open the port of <code>NodePort</code> services on all the nodes so that those services are available on :.</p>
<h3>2. Create an ingress</h3>
<p>An ingress is a Kubernetes resource that will allow traffic from outside the cluster to your services. We will now create such a resource to get an external IP and allow requests to our frontend service.</p>
<ol type="1">
<li>Open the file <a href="../../yaml/ingress.yaml" target="_blank">yaml/ingress.yaml</a><br>Notice that we have defined that we have configured our ingress to send requests to our <code>frontend</code> service on port <code>8080</code>.</li>
<li>Create the ingress resource:<pre><code>kubectl apply -f ./yaml/ingress.yaml
</code></pre>
</li>
<li>Wait for an external IP to be configured<pre><code>watch kubectl get ingress cv-ingress
</code></pre>
</li>
</ol>
<p>or</p>
<pre><code>  kubectl get ingress cv-ingress -w
</code></pre>
<p>It may take a few minutes for Kubernetes Engine to allocate an external IP address and set up forwarding rules until the load balancer is ready to serve your application. In the meanwhile, you may get errors such as HTTP 404 or HTTP 500 until the load balancer configuration is propagated across the globe.</p>
<ol type="1">
<li>Visit the external IP in your peferred browser to make sure you see your awezome CV online. If you get an error, the ingress and load balacing setup might not be completed.</li>
</ol>
<h3>3. Notes on exposing your application</h3>
<p>LoadBalancer type and the Ingress resource is dependent on your cloud provider. Google Cloud Platform supports these features, but other providers might not.</p>
<h2>2. Health checks</h2>
<p>Kubernetes uses health checks and readiness checks to figure out the state of the pods. If you don&#39;t define any health check, Kubernetes assumes it is . You can define your own.<br>If the health check responds with an error status code, Kubernetes will asume the container is unhealthy and kill the pod. Simliary, if the readiness check is unsuccessful, Kubernetes will asume it is not ready, and wait for it.</p>
<h2>3. Endpoint</h2>
<p>The first way to define a health check is to define which endpoint the check should use. Our backend application contains the endpoint <code>/healthz</code>. Go ahead and define this as the health-endpoint in the backend deployment file, under the container spec:</p>
<pre><code>livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
    httpHeaders:
    - name: X-Custom-Header
      value: Awesome
  initialDelaySeconds: 3
  periodSeconds: 3
</code></pre>


      </google-codelab-step>
    
  </google-codelab>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-49880327-14', 'auto');

    (function() {
      var gaCodelab = '';
      if (gaCodelab) {
        ga('create', gaCodelab, 'auto', {name: 'codelab'});
      }

      var gaView;
      var parts = location.search.substring(1).split('&');
      for (var i = 0; i < parts.length; i++) {
        var param = parts[i].split('=');
        if (param[0] === 'viewga') {
          gaView = param[1];
          break;
        }
      }
      if (gaView && gaView !== gaCodelab) {
        ga('create', gaView, 'auto', {name: 'view'});
      }
    })();
  </script>

</body>
</html>
